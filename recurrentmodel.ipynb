{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f42fdda",
   "metadata": {},
   "source": [
    "1.导入相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3060821c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c179a7e210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d170c",
   "metadata": {},
   "source": [
    "2.定义GPT的一些参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fdf4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 512\n",
    "    max_length: int = 512   # 这里其实应该是文本的最大长度（ max_seq_len）\n",
    "    batch_size: int = 4\n",
    "    n_layer: int = 2\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768    # n_embd 也叫 hidden_dim, hiden_size, 这里我同时设置了和 embed_dim 一样\n",
    "    head_size: int = n_embd // n_head\n",
    "    dropout: float = 0.1\n",
    "    # # tiktoken 使用的是 GPT-2 的词表，大约有 50257 个token\n",
    "    vocab_size: int = 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e397824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    # 单头注意力机制\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.value = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.query = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.head_size = config.head_size\n",
    "\n",
    "        # 尝试学习新的写法，attention_mask 通过 register_buffer 注册\n",
    "        # 因为不用计算 梯度，所以节约内存和显存，速度也更快\n",
    "        self.register_buffer(\n",
    "            'attention_mask', \n",
    "            torch.tril(\n",
    "                torch.ones(config.block_size, config.block_size)\n",
    "            ))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_size = x.size()\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        q = self.query(x)\n",
    "        weight = q @ k.transpose(-2, -1)   # @ 就是 torch.matmul 的简化写法\n",
    "        # 一定要在 softmax 前除以 sqrt(head_size)\n",
    "        weight = weight.masked_fill(\n",
    "            self.attention_mask[:seq_len, :seq_len] == 0, \n",
    "            float('-inf')\n",
    "        ) / math.sqrt(self.head_size)  # 这里的 hidden_size 其实是 head_size，因为是单头\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        weight = self.dropout(weight)#dropout的是attention weight，而不是乘完 value 之后的结果\n",
    "        out = weight @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config)\n",
    "                for _ in range(config.n_head)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat(\n",
    "            [h(x) for h in self.heads], \n",
    "            dim=-1\n",
    "        )\n",
    "        output = self.proj(output)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    # 实际上就是 MLP\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# 接下来就是一个完整的 Block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        head_size = config.n_embd // config.n_head\n",
    "        self.att = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# 以后会讲  MLA ,  MOE, DPO 完全手写\n",
    "# 完整的  GPT model\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "        self.ln_final = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # linear (4 -> 8)； weight shape 是记上是 8 * 4，\n",
    "        # 所以 embedding weight 和 lm_head weight 是共享的 非常重要！\n",
    "        # 这里学习一下 tie weight。\n",
    "        # 这是为了减少参数，加快训练；（现在 25的 SLM 很多都这样做了，注意⚠️）\n",
    "        # self.token_embedding_table.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 这里使用的是正态分布初始化\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx 是输入的 token ids\n",
    "        batch, seq_len = idx.size()\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        # seq 长度是这次输入的最大长度\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            # 要确保 位置编码和输入的 idx 在同一个设备上\n",
    "            torch.arange(seq_len, device=idx.device)\n",
    "        )\n",
    "        # 有一个经典题目：为什么 embedding 和 position 可以相加？ \n",
    "        x = token_emb + pos_emb   # shape is (batch, seq_len, n_embd)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)   # shape is (batch, seq_len, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_len, vocab_size = logits.size()\n",
    "            logits = logits.view(batch * seq_len, vocab_size)\n",
    "            targets = targets.view(batch * seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 如果序列太长，只取最后 block_size 个token\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # 获取预测\n",
    "            logits, _ = self(idx_cond)\n",
    "            # 只关注最后一个时间步的预测\n",
    "            logits = logits[:, -1, :]  # becomes (B, vocab_size)\n",
    "            # 应用softmax获取概率\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # 采样下一个token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # 附加到序列上\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a6aaa",
   "metadata": {},
   "source": [
    "4.构建输入的Dataset\n",
    "\n",
    "了解模型的输入是什么样子的（最重要）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c80c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path, config):\n",
    "        # 数据 mobvoi_seq_monkey_general_open_corpus.jsonl 中，\n",
    "        # 读取前 1000 行\n",
    "        import tiktoken\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.max_length = config.max_length\n",
    "\n",
    "        self.eos_token = self.enc.encode(\n",
    "            \"<|endoftext|>\",\n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]  # 数字代表是 50256\n",
    "        self.encoded_data = []\n",
    "        raw_data = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # 解析每一行的 JSON 对象\n",
    "                data = json.loads(line.strip())  # 使用 json.loads 解析单行\n",
    "                raw_data.append(data)  # 打印解析后的数据\n",
    "\n",
    "        full_encoded = []\n",
    "        for text in raw_data:\n",
    "            encoded_text = self.enc.encode(text)\n",
    "            full_encoded.extend(encoded_text + [self.eos_token])\n",
    "\n",
    "        # 将长文本分割成训练样本\n",
    "        for i in range(0, len(full_encoded), self.max_length):\n",
    "            # 多取一个 Token 作为目标\n",
    "            chunk = full_encoded[i:i + self.max_length + 1]\n",
    "            # 如果长度不够，用 eos_token 填充\n",
    "            if len(chunk) < self.max_length + 1:\n",
    "                chunk = chunk + [self.eos_token] * (self.max_length + 1 - len(chunk))\n",
    "            self.encoded_data.append(chunk)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.encoded_data[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"将文本编码为token IDs\"\"\"\n",
    "        return self.enc.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"将token IDs解码为文本\"\"\"\n",
    "        return self.enc.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346788a",
   "metadata": {},
   "source": [
    "5.运行相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5d4a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 3700\n",
      "Train size: 3330, Validation size: 370\n",
      "After split - Train size: 3330, Validation size: 370\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "config = GPTConfig()\n",
    "train_dataset = MyDataset(r'data\\mobvoi_seq_monkey_general_open_corpus_1000.json',config)\n",
    "\n",
    "# 打印数据集大小进行调试\n",
    "print(f\"Total dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# 获取数据集总大小\n",
    "total_size = len(train_dataset)\n",
    "\n",
    "# 确保数据集不为空\n",
    "if total_size == 0:\n",
    "    raise ValueError(\"数据集为空，请检查数据文件路径和数据加载过程\")\n",
    "\n",
    "# 计算训练集和验证集的大小\n",
    "train_size = int(0.9 * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "print(f\"Train size: {train_size}, Validation size: {val_size}\")\n",
    "\n",
    "# split traindataset to train and val\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# 再次检查分割后的数据集大小\n",
    "print(f\"After split - Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef5387d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:91.765248M\n"
     ]
    }
   ],
   "source": [
    "model=GPT(GPTConfig())\n",
    "device='cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "model=model.to(device)\n",
    "\n",
    "#打印模型一共有多少参数\n",
    "total_params=sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:{total_params / 1e6}M\")\n",
    "\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=3e-4)\n",
    "#设置cosine学习率\n",
    "scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87223b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 11.0185\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "def train(model, optimizer, scheduler, train_loader, val_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        # 将数据移到设备上\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        logits, loss = model(x, targets=y)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 调整学习率\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    return total_loss\n",
    "\n",
    "def eval(model, val_loader, device):\n",
    "    # 验证\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, targets=y)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "for epoch in range(2):\n",
    "    train_loss = train(model, optimizer, scheduler, train_loader, val_loader, device)\n",
    "    val_loss = eval(model, val_loader, device)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "    # 保存模型\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_loss': avg_val_loss,\n",
    "    }\n",
    "    # 保存每个epoch的模型\n",
    "    torch.save(checkpoint, f'checkpoints/model_epoch_{epoch}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
